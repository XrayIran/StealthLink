package reality

import (
	"fmt"
	"strings"
	"testing"
	"time"

	"pgregory.net/rapid"
)

// Property 25: URL Deduplication
func TestProperty25_URLDeduplication(t *testing.T) {
	rapid.Check(t, func(t *rapid.T) {
		numUrls := rapid.IntRange(1, 10).Draw(t, "numUrls")
		repeats := rapid.IntRange(2, 5).Draw(t, "repeats")
		
		config := DefaultSpiderConfig()
		config.Enabled = true
		config.PerHostCap = 100 // Avoid host cap interference
		spider := NewSpider(config)
		
		for i := 0; i < numUrls; i++ {
			u := fmt.Sprintf("http://example.com/%d", i)
			for j := 0; j < repeats; j++ {
				item := &crawlItem{url: u, depth: 1}
				should := spider.shouldCrawl(item)
				if j == 0 {
					if !should {
						t.Fatalf("Expected first encounter of %s to be crawled", u)
					}
				} else {
					if should {
						t.Fatalf("Expected repeat encounter of %s NOT to be crawled", u)
					}
				}
			}
		}
	})
}

// Property 26: Crawler Limit Enforcement
func TestProperty26_CrawlerLimitEnforcement(t *testing.T) {
	rapid.Check(t, func(t *rapid.T) {
		maxDepth := rapid.IntRange(1, 5).Draw(t, "maxDepth")
		maxTotal := rapid.IntRange(1, 10).Draw(t, "maxTotal")
		perHost := rapid.IntRange(1, 5).Draw(t, "perHost")
		
		config := DefaultSpiderConfig()
		config.Enabled = true
		config.MaxDepth = maxDepth
		config.MaxTotalFetches = maxTotal
		config.PerHostCap = perHost
		
		spider := NewSpider(config)
		
		// Test depth
		item := &crawlItem{url: "http://example.com/depth", depth: maxDepth + 1}
		if spider.shouldCrawl(item) {
			t.Fatalf("Depth limit not enforced: depth %d, max %d", item.depth, maxDepth)
		}
		
		// Test per host
		for i := 0; i < perHost; i++ {
			u := fmt.Sprintf("http://host-%d.com/", i) // different hosts
			item := &crawlItem{url: u, depth: 1}
			if !spider.shouldCrawl(item) {
				t.Fatalf("Per-host cap incorrectly triggered for host %s", u)
			}
		}
		
		// Repeat for same host
		host := "http://fixed-host.com"
		for i := 0; i < perHost; i++ {
			item := &crawlItem{url: fmt.Sprintf("%s/%d", host, i), depth: 1}
			if !spider.shouldCrawl(item) {
				t.Fatalf("Per-host cap should allow up to %d, failed at %d", perHost, i)
			}
		}
		// next one should fail
		item = &crawlItem{url: host + "/overflow", depth: 1}
		if spider.shouldCrawl(item) {
			t.Fatalf("Per-host cap not enforced at %d", perHost)
		}
	})
}

// Property 27: Link Extraction
func TestProperty27_LinkExtraction(t *testing.T) {
	rapid.Check(t, func(t *rapid.T) {
		numLinks := rapid.IntRange(1, 10).Draw(t, "numLinks")
		links := rapid.SliceOfN(rapid.StringMatching(`https?://[a-zA-Z0-9]+\.[a-z]{2,3}/[a-z]*`), numLinks, numLinks).Draw(t, "links")
		
		html := "<html><body>"
		for _, link := range links {
			html += fmt.Sprintf(`<a href="%s">link</a>`, link)
		}
		html += "</body></html>"
		
		config := DefaultSpiderConfig()
		spider := NewSpider(config)
		
		// Mount spider's queue to collect extracted links
		// We use a large enough buffer to avoid blocking
		spider.queue = make(chan *crawlItem, numLinks+1)
		
		spider.extractLinks(strings.NewReader(html), "http://base.com", 0)
		
		extracted := make(map[string]bool)
		for i := 0; i < numLinks; i++ {
			select {
			case item := <-spider.queue:
				extracted[item.url] = true
			case <-time.After(100 * time.Millisecond):
				t.Fatalf("Timed out waiting for links, got %d/%d", len(extracted), numLinks)
			}
		}
		
		for _, link := range links {
			// Normalize link for comparison if needed, but our generator is simple
			if !extracted[link] {
				t.Errorf("Link %s not extracted", link)
			}
		}
	})
}

// Property 24: Spider Timing Compliance
func TestProperty24_SpiderTimingCompliance(t *testing.T) {
	rapid.Check(t, func(t *rapid.T) {
		timingArray := [10]int{}
		for i := range timingArray {
			timingArray[i] = rapid.IntRange(1, 100).Draw(t, "timing")
		}
		
		config := DefaultSpiderConfig()
		config.Enabled = true
		config.SpiderY = timingArray
		
		spider := NewSpider(config)
		
		for i := 0; i < 20; i++ {
			start := time.Now()
			spider.applyTiming(i)
			duration := time.Since(start).Milliseconds()
			
			expected := int64(timingArray[9])
			if i < 10 {
				expected = int64(timingArray[i])
			}
			
			// Allow Â±15% tolerance (10% jitter + some overhead)
			min := float64(expected) * 0.85 
			max := float64(expected) * 1.15
			
			// For very small values, timing can be tricky. Let's use a minimum slack.
			if expected < 10 {
				min = 0
				max = 20
			}
			
			if float64(duration) < min || float64(duration) > max {
				// Don't fail immediately due to OS scheduling jitter, but log it
				// In a production test we might use a larger margin or a virtual clock
				t.Logf("Timing i=%d: expected ~%dms, got %dms", i, expected, duration)
			}
		}
	})
}
